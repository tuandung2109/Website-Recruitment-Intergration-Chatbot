import sys
import os
from pathlib import Path

# Add the backend directory to Python path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from mcp.server.fastmcp import FastMCP
from pymongo import MongoClient
from typing import List, Dict, Any
from setting import Settings


# 1Ô∏è‚É£ T·∫°o server
server = FastMCP("demo-mcp")

# Load settings
settings = Settings.load_settings()

# connect to MongoDB (example, adjust as needed)
mongo_client = MongoClient(settings.DATABASE_HOST)
db = mongo_client[settings.DATABASE_NAME]

# üöÄ Preload models ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô response
print("üöÄ Initializing models...")
from tool.model_manager import model_manager
model_manager.preload_models()
print("‚úÖ Models preloaded successfully!")

# 2Ô∏è‚É£ ƒê·ªãnh nghƒ©a tool
@server.tool()
def hello(name: str) -> str:
    """Say hello to a user"""
    return f"Hello, {name}!"

#MongoDB
@server.tool()
def find_documents(collection: str, query: Dict[str, Any] = {}) -> List[Dict[str, Any]]:
    """
    find documents in mongoDB
    Args:
        collection: name of the collection on mongoDB
        query: fillter query (vd: {"name": "Alice"})
    """
    col = db[collection]
    docs = list(col.find(query))
    
    for d in docs:
        d["_id"] = str(d["_id"])
    return docs

# Tool tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ c√¢u h·ªèi v·ªÅ JD
@server.tool()
def extract_features_from_question(query: str, prompt_type: str) -> Dict[str, Any]:
    """
    Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng t·ª´ c√¢u h·ªèi v·ªÅ JD
    Args:
        query: c√¢u h·ªèi c·ªßa user
        prompt_type: lo·∫°i prompt ƒë·ªÉ tr√≠ch xu·∫•t (vd: "extract_features_question_aboout_job")
    Returns:
        dict: c√°c ƒë·∫∑c tr∆∞ng ƒë√£ tr√≠ch xu·∫•t
    """
    from tool.extract_feature_question_about_jd import ExtractFeatureQuestion
    extractor = ExtractFeatureQuestion(
        model_name=os.getenv("OLLAMA_MODEL", "hf.co/Cactus-Compute/Qwen3-1.7B-Instruct-GGUF:Q4_K_M"),
        validate_response=["title", "skills", "company", "location", "experience"]
    )
    features = extractor.extract(query, prompt_type)
    return features


@server.tool()
def intent_classification(query: str) -> str:
    """
    Ph√¢n lo·∫°i intent c·ªßa c√¢u h·ªèi (s·ª≠ d·ª•ng cached models)
    Args:
        query: c√¢u h·ªèi c·ªßa user
    Returns:
        str: intent ƒë√£ ph√¢n lo·∫°i (vd: "recruitment", "salary", "company_info", ...)
    """
    from tool.model_manager import model_manager
    
    try:
        # L·∫•y semantic router t·ª´ cache
        semantic_router = model_manager.get_semantic_router()
        
        # Ph√¢n lo·∫°i intent
        print(f"\nüîç Classifying query: {query}")
        score, route_name = semantic_router.guide(query)
        print(f"‚úÖ Classification result: {route_name} (score: {score:.4f})")
        
        return route_name
        
    except Exception as e:
        print(f"‚ùå Error in intent classification: {str(e)}")
        return "unknown"

@server.tool()
def enhance_question(query: str) -> str:
    """
    N√¢ng c·∫•p c√¢u h·ªèi t·ª´ incomplete -> complete
    Args:
        query: c√¢u h·ªèi c·ªßa user
    Returns:
        str: c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p
    """
    from tool.question_enhancer import QuestionEnhancer
    enhancer = QuestionEnhancer()
    print(f"\nC√¢u h·ªèi: '{query}'")
    info_status = enhancer.analyze_incomplete_question(query)
    missing_info = enhancer.get_priority_missing_info(info_status)
    follow_up = enhancer.generate_follow_up_question(missing_info)
        
    print(f"Th√¥ng tin thi·∫øu: {[info.value for info in missing_info]}")
    print(f"C√¢u h·ªèi follow-up: {follow_up}")
    return follow_up


@server.tool()
def get_prompt(prompt_name: str, **kwargs) -> str:
    """
    L·∫•y prompt text. N·∫øu c√≥ kwargs th√¨ format c√°c placeholder.
    V√≠ d·ª•: get_prompt("extract_features_question_aboout_job", user_input="T√¨m vi·ªác ·ªü H√† N·ªôi")
    """
    from prompt.promt_config import PromptConfig
    prompt_config = PromptConfig()
    prompt_text = prompt_config.get_prompt(prompt_name, **kwargs)
    return prompt_text

@server.tool()
def get_reflection(history: List[Dict[str, str]]) -> str:
    """
    S·ª≠ d·ª•ng Reflection ƒë·ªÉ t·ª± ƒë√°nh gi√° v√† c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi
    Args:
        history: l·ªãch s·ª≠ h·ªôi tho·∫°i
        question: c√¢u h·ªèi hi·ªán t·∫°i
        max_iterations: s·ªë l·∫ßn l·∫∑p t·ªëi ƒëa ƒë·ªÉ c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi
    Returns:
        str: c√¢u tr·∫£ l·ªùi ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán
    """
    from tool.reflection import Reflection
    from llms.llm_manager import llm_manager
    
    # S·ª≠ d·ª•ng LLM Manager thay v√¨ t·∫°o instance m·ªõi
    default_url = "http://host.docker.internal:11434" if os.getenv("DOCKER_ENV") == "true" else "http://localhost:11434"
    ollama_url = os.getenv("OLLAMA_URL", default_url)
    ollama_model = os.getenv("OLLAMA_MODEL", "hf.co/Cactus-Compute/Qwen3-1.7B-Instruct-GGUF:Q4_K_M")
    
    # Reuse existing LLM instance t·ª´ manager
    llm = llm_manager.get_ollama_client(base_url=ollama_url, model_name=ollama_model)
    reflection = Reflection(llm=llm)
    
    try:
        improved_answer = reflection.__call__(history)
        if "<think>" in improved_answer:
                improved_answer = improved_answer.split("</think>")[-1].strip()
        print("Reflection completed.", {"improved_answer": improved_answer})
        return improved_answer
    except Exception as e:
        print(f"‚ùå Error in reflection process: {str(e)}")
        return "Error in reflection process."
    
@server.tool()
def search_similar_jobs(query_vector: List[float], top_k: int = 5):
    """
    T√¨m ki·∫øm c√°c JD t∆∞∆°ng t·ª± trong Qdrant d·ª±a tr√™n vector truy v·∫•n
    Args:
        query_vector: vector truy v·∫•n
        top_k: s·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
    Returns:
        list: danh s√°ch c√°c JD t∆∞∆°ng t·ª±
    """
    URL_QDRANT = settings.URL_QDRANT
    API_KEY_QDRANT = settings.API_KEY_QDRANT
    from tool.database import QDrant
    try:
        qdrant_client = QDrant(
            url=URL_QDRANT,
            api_key=API_KEY_QDRANT
        )
        results = qdrant_client.search_vectors(
            collection_name=settings.COLLECTION_JOB,
            query_vector=query_vector,
            top_k=top_k
        )
        print(f"‚úÖ Found {len(results)} similar jobs.")
        return results
    except Exception as e:
        print(f"‚ùå Error in searching similar jobs: {str(e)}")
        return []

# 3Ô∏è‚É£ Ch·∫°y server qua STDIO
if __name__ == "__main__":
    server.run()
